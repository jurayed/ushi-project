// services/transcription-service.js
const { OpenAI, toFile } = require('openai');
const path = require('path');
const fs = require('fs');

// –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è (–∫–ª—é—á –±–µ—Ä–µ—Ç—Å—è –∏–∑ .env –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
const openai = new OpenAI();

async function transcribeAudio(audioBuffer, filename = 'voice.webm') {
    if (!process.env.OPENAI_API_KEY) {
        throw new Error('OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env');
    }

    try {
        console.log(`üéôÔ∏è –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI SDK (${audioBuffer.length} –±–∞–π—Ç)...`);

        // OpenAI SDK —Ç—Ä–µ–±—É–µ—Ç File-like –æ–±—ä–µ–∫—Ç. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –±—É—Ñ–µ—Ä.
        const file = await toFile(audioBuffer, filename, {
            type: 'audio/webm'
        });

        const response = await openai.audio.transcriptions.create({
            file: file,
            model: "whisper-1",
            language: "ru", // –ü–æ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —è–∑—ã–∫ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
        });

        console.log('‚úÖ –£—Å–ø–µ—Ö:', response.text.substring(0, 30) + '...');
        
        return {
            text: response.text,
            language: response.language || 'ru'
        };

    } catch (error) {
        console.error('‚ùå –û—à–∏–±–∫–∞ OpenAI Whisper:', error);
        throw new Error(`–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: ${error.message}`);
    }
}

module.exports = { transcribeAudio };
