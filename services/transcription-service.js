// services/transcription-service.js
const { OpenAI, toFile } = require('openai');
const path = require('path');

const openai = new OpenAI();

async function transcribeAudio(audioBuffer, filename = 'voice.webm') {
    if (!process.env.OPENAI_API_KEY) {
        throw new Error('OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env');
    }

    // –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –±—É—Ñ–µ—Ä–∞ (–º–µ–Ω—å—à–µ 1–ö–ë ‚Äî —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ —Ç–∏—à–∏–Ω–∞ –∏–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫)
    if (audioBuffer.length < 1000) {
        console.warn('‚ö†Ô∏è –ê—É–¥–∏–æ—Ñ–∞–π–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ OpenAI');
        return { text: "...", language: "ru" }; // –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
    }

    try {
        console.log(`üéôÔ∏è –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è (${audioBuffer.length} –±–∞–π—Ç)...`);

        const file = await toFile(audioBuffer, filename, { type: 'audio/webm' });

        const response = await openai.audio.transcriptions.create({
            file: file,
            model: "whisper-1",
            language: "ru",
        });

        console.log('‚úÖ –£—Å–ø–µ—Ö:', response.text.substring(0, 30));
        
        return {
            text: response.text,
            language: response.language || 'ru'
        };

    } catch (error) {
        // –ï—Å–ª–∏ OpenAI —Ä—É–≥–∞–µ—Ç—Å—è –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–π —Ñ–∞–π–ª - –Ω–µ —Å—á–∏—Ç–∞–µ–º —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–æ–π
        if (error.message && error.message.includes('too short')) {
            console.warn('‚ö†Ô∏è OpenAI: –ê—É–¥–∏–æ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ.');
            return { text: "", language: "ru" };
        }

        console.error('‚ùå –û—à–∏–±–∫–∞ OpenAI Whisper:', error.message);
        throw new Error(`–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: ${error.message}`);
    }
}

module.exports = { transcribeAudio };
